{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory of the \"notebooks\" directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WebPageLinkExtractor` is a Python class that extracts all links from a given webpage recursively up to a certain depth. The class takes a base URL as input along with optional parameters such as the maximum depth of recursion and the timeout value for fetching pages.\n",
    "\n",
    "The class uses the `requests` library to fetch the content of the webpage and `BeautifulSoup` library to extract all links from the HTML content. The class then follows each link that belongs to the same domain as the base URL, and extracts all links from each of those pages as well. This process continues recursively until the maximum depth of recursion is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loaders.websites import WebPageLinkExtractor\n",
    "\n",
    "base_url = 'https://python.langchain.com/en/latest/'\n",
    "extractor = WebPageLinkExtractor(base_url, max_depth=1000)\n",
    "links = extractor.get_links()\n",
    "print(f'\\nMaximum depth reached: {extractor.current_max_depth}')\n",
    "print(f'Total links found: {len(links)}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now filter only `html` links as the above code downloads all the links like `.md` or `.pynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_links = [link for link in links if link.endswith('.html')]\n",
    "print(len(html_links))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nest_asyncio library is used in Python to enable running asyncio event loops inside a Jupyter notebook environment. This library is used because Jupyter notebooks use their own event loop, which can cause conflicts when trying to run other event loops like asyncio.\n",
    "\n",
    "When using asyncio in a Jupyter notebook, nest_asyncio.apply() is used to patch the event loop and allow it to run in the notebook environment. This essentially allows the asyncio event loop to run inside the notebook's event loop without conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `WebBaseLoader` class provided by the `langchain` package to asynchronously load all the html contents from the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(html_links)\n",
    "loader.requests_per_second = 10\n",
    "html_web_pages = loader.aload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(html_web_pages))\n",
    "print(html_web_pages[0].page_content)\n",
    "print(html_web_pages[0])\n",
    "print(html_web_pages[0].metadata)\n",
    "print(html_web_pages[0].metadata.get(\"source\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Splitters\n",
    "\n",
    "Note that for the tokenizer we defined the encoder as `\"cl100k_base\"`. This is a specific tiktoken encoder which is used by `gpt-3.5-turbo`. Other encoders exist and at the time of writing are summarized as:\n",
    "\n",
    "| Encoder | Models |\n",
    "| --- | --- |\n",
    "| `cl100k_base` | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002` |\n",
    "| `p50k_base` | `text-davinci-003`, `code-davinci-002`, `code-cushman-002` |\n",
    "| `r50k_base` | `text-davinci-001`, `davinci`, `text-similarity-davinci-001` |\n",
    "| `gpt2` | `gpt2` |\n",
    "\n",
    "You can find these details in the [Tiktoken `model.py` script](https://github.com/openai/tiktoken/blob/main/tiktoken/model.py), or using `tiktoken.encoding_for_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the text for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_pages_text_chunks = text_splitter.split_documents(html_web_pages)\n",
    "print(len(web_pages_text_chunks))\n",
    "print(web_pages_text_chunks[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings using `OpenAIEmbeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=\"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\")\n",
    "temp_text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(temp_text)\n",
    "print(query_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Qdrant vector store to save the embeddings locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "qdrant_url = \"http://localhost:6333/\"\n",
    "\n",
    "qdrant = Qdrant.from_documents(documents=web_pages_text_chunks[1:5],\n",
    "                               embedding=embeddings, \n",
    "                               url=qdrant_url, \n",
    "                               collection_name=\"langchain_documents\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the OpenAI API key as an environment variable in your system. In Linux or macOS, you can do this by running the following command in a terminal: \n",
    "\n",
    "`export OPENAI_API_KEY=<your_key_here>`.\n",
    "\n",
    "Restart your Jupyter notebook to ensure the environment variable is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import qdrant_client\n",
    "\n",
    "collection_name = \"langchain_documents\"\n",
    "qdrant_url = \"http://localhost:6333/\"\n",
    "qdrant_port = 6333\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "query = \"What wrappers are provided by SearxNG search API\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval\n",
    "\n",
    "Similarity search\n",
    "The simplest scenario for using Qdrant vector store is to perform a similarity search. Under the hood, our query will be encoded with the embedding_function and used to find similar documents in Qdrant collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(url=qdrant_url, port=qdrant_port)\n",
    "\n",
    "qdrant = Qdrant(client=client, \n",
    "                collection_name=\"langchain_documents\", \n",
    "                embedding_function=embeddings.embed_query)\n",
    "\n",
    "found_docs = qdrant.similarity_search(query)\n",
    "print(found_docs[0].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we might want to perform the search, but also obtain a relevancy score to know how good is a particular result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_docs = qdrant.similarity_search_with_score(query)\n",
    "document, score = found_docs[0]\n",
    "print(document.page_content)\n",
    "print(f\"\\nScore: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "\n",
    "client = QdrantClient(url=qdrant_url)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai_api_key)\n",
    "qdrant = Qdrant(client=client, collection_name=collection_name, embedding_function=embeddings.embed_query)\n",
    "search_results = qdrant.similarity_search(query, k=2)\n",
    "chain = load_qa_chain(OpenAI(openai_api_key=openai_api_key,temperature=0.2), chain_type=\"stuff\")\n",
    "results = chain({\"input_documents\": search_results, \"question\": query}, return_only_outputs=True)\n",
    "\n",
    "print(results[\"output_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "659c28ce7e3ad1cd8a0d13fd6d5a8530fe6cc749aa1f817ccd73612dea7b9ea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
